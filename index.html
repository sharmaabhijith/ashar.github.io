<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Abhijith Sharma</title> <meta name="author" content="Abhijith Sharma"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sharmaabhijith.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/media/">media</a> </li> <li class="nav-item "> <a class="nav-link" href="/studies/">studies</a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/cv_slyman.pdf">cv (pdf)</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Abhijith</span> Sharma </h1> <p class="desc">AI/CS Ph.D Student, Oregon State University</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/me-480.webp 480w, /assets/img/me-800.webp 800w, /assets/img/me-1400.webp 1400w, " sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"></source> <img src="/assets/img/me.jpeg?274b410709ff5176fb3a970812b660bf" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="me.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> </div> </div> <div class="clearfix"> <p>Hi Folks! I am a Ph.D. student at the intersection of multimodal AI, human-computer interaction, and fairness in the <a href="https://eecs.oregonstate.edu/ai-degree-program" rel="external nofollow noopener" target="_blank">Artificial Intelligence</a> and <a href="https://eecs.oregonstate.edu/academics/graduate/cs" rel="external nofollow noopener" target="_blank">Computer Science</a> programs at Oregon State University, where I’m advised by <a href="https://web.engr.oregonstate.edu/~leestef/" rel="external nofollow noopener" target="_blank">Stefan Lee</a> (and <a href="https://minsuk.com/" rel="external nofollow noopener" target="_blank">Minsuk Kahng</a> before his relocation to Google).</p> <p>My research evaluates large-scale vision-language models like CLIP, ViLBERT, LLaVA, and DALL-E. I enjoy auditing the real-world steps to get these models in production and determining mitigations to help promote fair outcomes. Frequently, this leads to human-centered methods that consider the entire lifecycle of a model: from data collection to deployment.</p> <p>In addition to research, I am also the Co-President of OSU’s <a href="https://www.aigsa.club/" rel="external nofollow noopener" target="_blank">AI Graduate Student Association</a>, where I organize social and professional events for our AI community and run the <a href="https://www.aigsa.club/aiasp/" rel="external nofollow noopener" target="_blank">AI Application Support Program</a> to provide application advising assistance to underrepresented people applying to graduate school.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Feb 27, 2024</th> <td> Our paper <em>“<a href="/fairdedup">FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication</a>”</em> was accepted at <a href="https://cvpr.thecvf.com/" rel="external nofollow noopener" target="_blank">CVPR 2024</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 7, 2024</th> <td> My research on AI Fairness was selected as a Featured Program for the 2024 State of Diversity at Oregon State. </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 11, 2023</th> <td> Our paper “<a href="https://ericslyman.com/assets/pdf/valet.pdf" rel="external nofollow noopener" target="_blank">VALET: Vision-And-LanguagE Testing with Reusable Components</a>” was accepted at the <a href="https://www.queerinai.com/neurips-2023" rel="external nofollow noopener" target="_blank">NeurIPS 2023 Queer in AI Workshop</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 1, 2023</th> <td> My work was featured in the article “<a href="https://diversity.oregonstate.edu/sites/diversity.oregonstate.edu/files/2024_oid_newsletter_fall-final_web.pdf" rel="external nofollow noopener" target="_blank">Adressing Bias in AI</a>” in the magazine <em>Taking Action</em> from the OSU Office of Insitituional Diversity. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 25, 2023</th> <td> I was invited to share my work on <a href="/vlslice">VLSlice</a> with the <a href="https://pair.withgoogle.com/" rel="external nofollow noopener" target="_blank">People+AI Research Group</a> (PAIR) at Google. </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/fairdedup-480.webp 480w, /assets/img/publication_preview/fairdedup-800.webp 800w, /assets/img/publication_preview/fairdedup-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/fairdedup.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fairdedup.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="slyman2024fairdedup" class="col-sm-8"> <div class="title">FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication</div> <div class="author"> <em>Eric Slyman</em>, <a href="https://web.engr.oregonstate.edu/~leestef/" rel="external nofollow noopener" target="_blank">Stefan Lee</a>, <a href="https://research.adobe.com/person/scott-cohen/" rel="external nofollow noopener" target="_blank">Scott Cohen</a>, and <a href="https://kushalkafle.com/" rel="external nofollow noopener" target="_blank">Kushal Kafle</a> </div> <div class="periodical"> <em>Computer Vision and Patern Recognition</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2404.16123" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="/assets/pdf/fairdedup_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/fairdedup/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web – datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">slyman2024fairdedup</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Slyman, Eric and Lee, Stefan and Cohen, Scott and Kafle, Kushal}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Patern Recognition}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://www.youtube.com/watch?v=6EfEM-hgIn8" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/valet-480.webp 480w, /assets/img/publication_preview/valet-800.webp 800w, /assets/img/publication_preview/valet-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/valet.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="valet.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="slyman2023valet" class="col-sm-8"> <div class="title">VALET: Vision-And-LanguagE Testing with Reusable Components</div> <div class="author"> <em>Eric Slyman</em>, <a href="https://kushalkafle.com/" rel="external nofollow noopener" target="_blank">Kushal Kafle</a>, and <a href="https://research.adobe.com/person/scott-cohen/" rel="external nofollow noopener" target="_blank">Scott Cohen</a> </div> <div class="periodical"> <em>NeurIPS Queer in AI Workshop</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/valet.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="/assets/pdf/valet_poster.jpg" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Vision-and-Language (ViL) modeling advancements have resulted in significant improvements to aggregate metric performance on a variety of tasks. However, this evaluation may not accurately reflect a model’s capability to behave as intended by its creator, or according to the expectations of an end-user. Behavioral testing and sensemaking methods have been identified as effective for surfacing these errors in ViL models, but are limited in practice by their ability to scale to many examples and involved engineering requirements. In order to be practical for ViL tasks and suitable for organizational-level testing, these methods must scale to large sample sizes without requiring costly or repetitive engineering efforts for each individual test case. To address these challenges, we propose VALET, a system designed to rapidly develop scalable behavioral tests for ViL models that offers a high-level interface for non-technical users to perform testing that is supported by a modular system of interoperable components enabling expert users to extend and share testing environments more easily. We present a case study using VALET to evaluate a language-guided model’s capability to count in zero-shot image classification.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">slyman2023valet</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Slyman, Eric and Kafle, Kushal and Cohen, Scott}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VALET: Vision-And-LanguagE Testing with Reusable Components}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{NeurIPS Queer in AI Workshop}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://drive.google.com/file/d/11d2UuUuh7Cc6yOp7uEGUTQ9y1FEPzRjl/preview" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/vlslice-480.webp 480w, /assets/img/publication_preview/vlslice-800.webp 800w, /assets/img/publication_preview/vlslice-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/vlslice.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vlslice.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="slyman2023vlslice" class="col-sm-8"> <div class="title">VLSlice: Interactive Vision-and-Language Slice Discovery</div> <div class="author"> <em>Eric Slyman</em>, <a href="https://minsuk.com/" rel="external nofollow noopener" target="_blank">Minsuk Kahng</a>, and <a href="https://web.engr.oregonstate.edu/~leestef/" rel="external nofollow noopener" target="_blank">Stefan Lee</a> </div> <div class="periodical"> <em>International Conference on Computer Vision</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2309.06703" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="https://github.com/slymane/vlslice" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/vlslice_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/vlslice/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond "tabular" data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">slyman2023vlslice</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Slyman, Eric and Kahng, Minsuk and Lee, Stefan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VLSlice: Interactive Vision-and-Language Slice Discovery}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Computer Vision}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{15291-15301}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://drive.google.com/file/d/1mOuvjphNb2xNDC7shoGbPwyjbfArwud4/preview" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%61%62%68%69%6A%69%74%68.%73%68%61%72%6D%61.%75%62%63@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-2481-7942" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=HruWYeIAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/sharmaabhijith" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/https://www.linkedin.com/in/abhijith-sharma/" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/EricSlyman" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://instagram.com/abhijith_vu2" title="Instagram" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-instagram"></i></a> </div> <div class="contact-note"> Reach me via email at slymane[at]oregonstate[dot]edu </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Abhijith Sharma. Last updated: July 29, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>